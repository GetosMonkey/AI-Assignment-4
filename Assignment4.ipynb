{
 "cells": [
  {
   "cell_type": "raw",
   "id": "a1483c2f-a33b-4585-b4e0-b4e79568dd2a",
   "metadata": {},
   "source": [
    "# CSC2042S 2025 \n",
    "## Assignment 3 \n",
    "\n",
    "## Optimization of a neural network for image classification \n",
    "#### Maryam Abrahams (ABRMAR043)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe7fc9e-d7f2-43e1-8899-f4941aa3a5d1",
   "metadata": {},
   "source": [
    "## Task 1: Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c59b4218-abd5-4be1-ae68-1c9e3f66bd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Imports and setup \n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random as random\n",
    "\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Reproducibility setup\n",
    "\n",
    "seed = 69\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dff462be-42e1-4d81-b3ac-ed05bd930ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full training set split into:\n",
      "  - New Training Set:   50000 images\n",
      "  - Validation Set: 10000 images\n",
      "  - Test Set:         10000 images\n",
      "Data processing complete. DataLoaders are ready.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Defining the transformations\n",
    "transform = Compose([\n",
    "    ToTensor(),\n",
    "    Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# downloading all data\n",
    "full_train_dataset = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# validation, test and train splits\n",
    "\n",
    "train_size = 50000\n",
    "val_size = len(full_train_dataset) - train_size\n",
    "\n",
    "train_dataset, validation_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
    "\n",
    "print(f\"Full training set split into:\")\n",
    "print(f\"  - New Training Set:   {len(train_dataset)} images\")\n",
    "print(f\"  - Validation Set: {len(validation_dataset)} images\")\n",
    "\n",
    "test_dataset = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "print(f\"  - Test Set:         {len(test_dataset)} images\")\n",
    "\n",
    "# creating dataLoader objects\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(\"Data processing complete. DataLoaders are ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b230fd26-6c9f-43d8-b28e-f809818cb26f",
   "metadata": {},
   "source": [
    "## Task 2: Building and Training a Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b046dc2-675f-4fcc-99f0-ec9cb1b25c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# flexible mlp class for training\n",
    "\n",
    "class FlexibleMLP(nn.Module):\n",
    "    def __init__(self, hidden1=128, hidden2=64, dropout=0.0, activation='relu'):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # Choose activation function\n",
    "        if activation.lower() == 'sigmoid':\n",
    "            self.act = nn.Sigmoid()\n",
    "        else: # Default to ReLU as required by baseline\n",
    "            self.act = nn.ReLU()\n",
    "        \n",
    "        # Build network layers\n",
    "        self.fc1 = nn.Linear(28*28, hidden1)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(hidden1, hidden2)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.fc3 = nn.Linear(hidden2, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.act(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.act(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb77bf4c-bf71-4bc1-946a-bf7610337b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Main training loop\n",
    "\n",
    "def train_model(model, train_loader, val_loader, loss_fn, optimizer, epochs=10):\n",
    "    \"\"\"\n",
    "    Train model and track training/validation losses per epoch\n",
    "    Returns: dict with train_losses, val_losses, and final_accuracy\n",
    "    \"\"\"\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        epoch_train_loss = 0\n",
    "        for X, y in train_loader:\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_train_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = epoch_train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        epoch_val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X, y in val_loader:\n",
    "                pred = model(X)\n",
    "                loss = loss_fn(pred, y)\n",
    "                epoch_val_loss += loss.item()\n",
    "                \n",
    "                correct += (pred.argmax(1) == y).sum().item()\n",
    "                total += y.size(0)\n",
    "        \n",
    "        avg_val_loss = epoch_val_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        accuracy = 100 * correct / total\n",
    "        \n",
    "        if (epoch + 1) % 2 == 0 or epoch == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {avg_val_loss:.4f}, Val Acc: {accuracy:.2f}%\")\n",
    "    \n",
    "    final_accuracy = 100 * correct / total\n",
    "    return {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'final_accuracy': final_accuracy,\n",
    "        'model': model\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdc1e364-0119-4845-be58-9cde6cc201bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # sequence of layers\n",
    "        self.network_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 128), # first hidden layer\n",
    "            nn.ReLU(),             # ReLU \n",
    "            nn.Linear(128, 64),    # second hidden layer \n",
    "            nn.ReLU(),             # ReLU \n",
    "            nn.Linear(64, 10)      # Output\n",
    "        )\n",
    "\n",
    "    # forward pass\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.network_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4211cbf7-fbb8-4b91-9b4a-9fb025ce2056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRAINING BASELINE MODEL\n",
      "============================================================\n",
      "Epoch 1/10 - Train Loss: 0.5352, Val Loss: 0.4001, Val Acc: 85.26%\n",
      "Epoch 2/10 - Train Loss: 0.3893, Val Loss: 0.3620, Val Acc: 86.35%\n",
      "Epoch 4/10 - Train Loss: 0.3273, Val Loss: 0.3476, Val Acc: 87.09%\n",
      "Epoch 6/10 - Train Loss: 0.2908, Val Loss: 0.3273, Val Acc: 88.15%\n",
      "Epoch 8/10 - Train Loss: 0.2624, Val Loss: 0.3263, Val Acc: 88.09%\n",
      "Epoch 10/10 - Train Loss: 0.2423, Val Loss: 0.3198, Val Acc: 88.29%\n",
      "\n",
      " Baseline Model Final Accuracy: 88.29%\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"TRAINING BASELINE MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "baseline_model = BaselineMLP()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(baseline_model.parameters(), lr=0.001)\n",
    "\n",
    "baseline_results = train_model(\n",
    "    baseline_model, \n",
    "    train_loader, \n",
    "    validation_loader, \n",
    "    loss_fn, \n",
    "    optimizer, \n",
    "    epochs=10 \n",
    ")\n",
    "\n",
    "baseline_accuracy = baseline_results['final_accuracy']\n",
    "print(f\"\\n Baseline Model Final Accuracy: {baseline_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb9a91a-e928-488b-a934-9e5c75f39ecf",
   "metadata": {},
   "source": [
    "## Task 3: Hyperparameter Optimization Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "303087e1-df53-4b2d-a23a-7d7820ca0bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = []\n",
    "best_accuracy = 0\n",
    "best_config = None\n",
    "best_model_results = None\n",
    "\n",
    "# Baseline configuration\n",
    "base_lr = 0.001\n",
    "base_opt = torch.optim.Adam\n",
    "base_h1 = 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6a6970c-47f5-4e95-97a0-daf2f4eff135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- EXPERIMENT 1: Learning Rate ---\n",
      "\n",
      " Training: LR=0.1\n",
      "Epoch 1/5 - Train Loss: 2.6034, Val Loss: 2.3058, Val Acc: 9.59%\n",
      "Epoch 2/5 - Train Loss: 2.3122, Val Loss: 2.3138, Val Acc: 9.64%\n",
      "Epoch 4/5 - Train Loss: 2.3124, Val Loss: 2.3113, Val Acc: 9.59%\n",
      "\n",
      " Training: LR=0.01\n",
      "Epoch 1/5 - Train Loss: 0.5569, Val Loss: 0.4874, Val Acc: 82.49%\n",
      "Epoch 2/5 - Train Loss: 0.4487, Val Loss: 0.4787, Val Acc: 83.94%\n",
      "Epoch 4/5 - Train Loss: 0.4099, Val Loss: 0.4278, Val Acc: 85.64%\n",
      "\n",
      " Training: LR=0.001\n",
      "Epoch 1/5 - Train Loss: 0.5358, Val Loss: 0.4214, Val Acc: 85.06%\n",
      "Epoch 2/5 - Train Loss: 0.3876, Val Loss: 0.3649, Val Acc: 86.32%\n",
      "Epoch 4/5 - Train Loss: 0.3226, Val Loss: 0.3447, Val Acc: 87.08%\n",
      "\n",
      " Training: LR=0.0001\n",
      "Epoch 1/5 - Train Loss: 0.8424, Val Loss: 0.5464, Val Acc: 80.60%\n",
      "Epoch 2/5 - Train Loss: 0.5010, Val Loss: 0.4690, Val Acc: 83.25%\n",
      "Epoch 4/5 - Train Loss: 0.4223, Val Loss: 0.4213, Val Acc: 84.81%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# EXPERIMENT 1: learning rates\n",
    "print(\"\\n--- EXPERIMENT 1: Learning Rate ---\")\n",
    "learning_rates = [0.1, 0.01, 0.001, 0.0001]\n",
    "\n",
    "for lr in learning_rates:\n",
    "    config = f\"LR={lr}\"\n",
    "    print(f\"\\n Training: {config}\")\n",
    "    \n",
    "    model = FlexibleMLP(hidden1=base_h1, hidden2=64)\n",
    "    optimizer = base_opt(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    \n",
    "    result = train_model(model, train_loader, validation_loader, \n",
    "                         loss_fn, optimizer, epochs=10)\n",
    "    \n",
    "    results.append({\n",
    "        'experiment': 'Learning Rate',\n",
    "        'hyperparameter': f'lr={lr}',\n",
    "        'learning_rate': lr,\n",
    "        'optimizer': 'Adam',\n",
    "        'hidden1_neurons': base_h1,\n",
    "        'final_accuracy': result['final_accuracy'],\n",
    "        'train_losses': result['train_losses'],\n",
    "        'val_losses': result['val_losses']\n",
    "    })\n",
    "    \n",
    "    if result['final_accuracy'] > best_accuracy:\n",
    "        best_accuracy = result['final_accuracy']\n",
    "        best_config = config\n",
    "        best_model_results = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3c28b45-65a9-4f4a-8342-98f90a2e50b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- EXPERIMENT 2: Optimizer ---\n",
      "\n",
      " Training: Optimizer=Adam\n",
      "Epoch 1/5 - Train Loss: 0.5387, Val Loss: 0.4247, Val Acc: 84.21%\n",
      "Epoch 2/5 - Train Loss: 0.3948, Val Loss: 0.3800, Val Acc: 85.69%\n",
      "Epoch 4/5 - Train Loss: 0.3256, Val Loss: 0.3601, Val Acc: 86.28%\n",
      "\n",
      " Training: Optimizer=SGD\n",
      "Epoch 1/5 - Train Loss: 2.1517, Val Loss: 1.9572, Val Acc: 40.19%\n",
      "Epoch 2/5 - Train Loss: 1.6978, Val Loss: 1.4478, Val Acc: 60.48%\n",
      "Epoch 4/5 - Train Loss: 0.9990, Val Loss: 0.9150, Val Acc: 72.11%\n",
      "\n",
      " Training: Optimizer=RMSprop\n",
      "Epoch 1/5 - Train Loss: 0.5247, Val Loss: 0.4888, Val Acc: 81.37%\n",
      "Epoch 2/5 - Train Loss: 0.3877, Val Loss: 0.3767, Val Acc: 85.63%\n",
      "Epoch 4/5 - Train Loss: 0.3254, Val Loss: 0.3755, Val Acc: 85.84%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# EXPERIMENT 2: optimizers\n",
    "print(\"\\n--- EXPERIMENT 2: Optimizer ---\")\n",
    "optimizers_dict = {\n",
    "    'Adam': torch.optim.Adam,\n",
    "    'SGD': torch.optim.SGD,\n",
    "    'RMSprop': torch.optim.RMSprop\n",
    "}\n",
    "\n",
    "for opt_name, opt_class in optimizers_dict.items():\n",
    "    config = f\"Optimizer={opt_name}\"\n",
    "    print(f\"\\n Training: {config}\")\n",
    "    \n",
    "    model = FlexibleMLP(hidden1=base_h1, hidden2=64)\n",
    "    optimizer = opt_class(model.parameters(), lr=base_lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    \n",
    "    result = train_model(model, train_loader, validation_loader, \n",
    "                         loss_fn, optimizer, epochs=10)\n",
    "    \n",
    "    results.append({\n",
    "        'experiment': 'Optimizer',\n",
    "        'hyperparameter': opt_name,\n",
    "        'learning_rate': base_lr,\n",
    "        'optimizer': opt_name,\n",
    "        'hidden1_neurons': base_h1,\n",
    "        'final_accuracy': result['final_accuracy'],\n",
    "        'train_losses': result['train_losses'],\n",
    "        'val_losses': result['val_losses']\n",
    "    })\n",
    "    \n",
    "    if result['final_accuracy'] > best_accuracy:\n",
    "        best_accuracy = result['final_accuracy']\n",
    "        best_config = config\n",
    "        best_model_results = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f50e1b-7020-4c0b-9fb9-70d976f65a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- EXPERIMENT 3: Hidden Layer 1 Neurons ---\n",
      "\n",
      " Training: Hidden1=64\n",
      "Epoch 1/5 - Train Loss: 0.5545, Val Loss: 0.4607, Val Acc: 83.16%\n",
      "Epoch 2/5 - Train Loss: 0.4037, Val Loss: 0.3888, Val Acc: 85.42%\n",
      "Epoch 4/5 - Train Loss: 0.3394, Val Loss: 0.3756, Val Acc: 85.99%\n",
      "\n",
      " Training: Hidden1=128\n",
      "Epoch 1/5 - Train Loss: 0.5381, Val Loss: 0.4090, Val Acc: 84.99%\n",
      "Epoch 2/5 - Train Loss: 0.3945, Val Loss: 0.3717, Val Acc: 86.29%\n",
      "Epoch 4/5 - Train Loss: 0.3277, Val Loss: 0.3421, Val Acc: 86.78%\n",
      "\n",
      " Training: Hidden1=256\n",
      "Epoch 1/5 - Train Loss: 0.5181, Val Loss: 0.4453, Val Acc: 83.58%\n",
      "Epoch 2/5 - Train Loss: 0.3799, Val Loss: 0.3604, Val Acc: 86.81%\n",
      "Epoch 4/5 - Train Loss: 0.3154, Val Loss: 0.3447, Val Acc: 86.95%\n",
      "\n",
      " Training: Hidden1=512\n",
      "Epoch 1/5 - Train Loss: 0.5014, Val Loss: 0.3999, Val Acc: 85.67%\n",
      "Epoch 2/5 - Train Loss: 0.3749, Val Loss: 0.3611, Val Acc: 86.48%\n"
     ]
    }
   ],
   "source": [
    "# EXPErIMENT 3: neuron counts\n",
    "print(\"\\n--- EXPERIMENT 3: Hidden Layer 1 Neurons ---\")\n",
    "hidden1_neurons = [64, 128, 256, 512]\n",
    "\n",
    "for h1 in hidden1_neurons:\n",
    "    config = f\"Hidden1={h1}\"\n",
    "    print(f\"\\n Training: {config}\")\n",
    "    \n",
    "    model = FlexibleMLP(hidden1=h1, hidden2=64)\n",
    "    optimizer = base_opt(model.parameters(), lr=base_lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    \n",
    "    result = train_model(model, train_loader, validation_loader, \n",
    "                         loss_fn, optimizer, epochs=10)\n",
    "    \n",
    "    results.append({\n",
    "        'experiment': 'Hidden Neurons',\n",
    "        'hyperparameter': f'h1={h1}',\n",
    "        'learning_rate': base_lr,\n",
    "        'optimizer': 'Adam',\n",
    "        'hidden1_neurons': h1,\n",
    "        'final_accuracy': result['final_accuracy'],\n",
    "        'train_losses': result['train_losses'],\n",
    "        'val_losses': result['val_losses']\n",
    "    })\n",
    "    \n",
    "    if result['final_accuracy'] > best_accuracy:\n",
    "        best_accuracy = result['final_accuracy']\n",
    "        best_config = config\n",
    "        best_model_results = result\n",
    "\n",
    "print(f\"\\n Best Configuration: {best_config}\")\n",
    "print(f\" Best Accuracy: {best_accuracy:.2f}% (Baseline: {baseline_accuracy:.2f}%)\")\n",
    "print(f\" Total experiments run: {len(results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3688e91-c115-494a-b3f0-936cd5d4af19",
   "metadata": {},
   "source": [
    "## Task 4: Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebb1594-3fce-40bc-8f2b-73bb6599c684",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# table of results\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values('final_accuracy', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RESULTS TABLE\")\n",
    "print(\"=\"*60)\n",
    "print(results_df[['learning_rate', 'optimizer', 'hidden1_neurons', 'final_accuracy']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55628dd6-2f02-4e2e-b601-c9c3c55fbf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hyperparameter impact bar chart\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "experiments = ['Learning Rate', 'Optimizer', 'Hidden Neurons']\n",
    "\n",
    "for idx, exp_name in enumerate(experiments):\n",
    "    exp_data = results_df[results_df['experiment'] == exp_name]\n",
    "    \n",
    "    axes[idx].bar(range(len(exp_data)), exp_data['final_accuracy'], \n",
    "                  color=['lime' if acc == exp_data['final_accuracy'].max() \n",
    "                         else 'cornflowerblue' for acc in exp_data['final_accuracy']])\n",
    "    axes[idx].axhline(y=baseline_accuracy, color='deeppink', linestyle='--', \n",
    "                      label=f'Baseline ({baseline_accuracy:.2f}%)', linewidth=2)\n",
    "    axes[idx].set_xticks(range(len(exp_data)))\n",
    "    axes[idx].set_xticklabels(exp_data['hyperparameter'], rotation=45, ha='right')\n",
    "    axes[idx].set_ylabel('Validation Accuracy (%)')\n",
    "    axes[idx].set_title(f'{exp_name} Impact')\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b60b50a-6fc3-441f-8e3c-daf73824ff4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Loss curves \n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "epochs_range_baseline = range(1, len(baseline_results['train_losses']) + 1)\n",
    "plt.plot(epochs_range_baseline, baseline_results['train_losses'], \n",
    "         'b-', label='Baseline Train Loss', linewidth=2)\n",
    "plt.plot(epochs_range_baseline, baseline_results['val_losses'], \n",
    "         'b--', label='Baseline Val Loss', linewidth=2)\n",
    "\n",
    "# Plot best model\n",
    "epochs_range_best = range(1, len(best_model_results['train_losses']) + 1)\n",
    "plt.plot(epochs_range_best, best_model_results['train_losses'], \n",
    "         'g-', label='Best Model Train Loss', linewidth=2)\n",
    "plt.plot(epochs_range_best, best_model_results['val_losses'], \n",
    "         'g--', label='Best Model Val Loss', linewidth=2)\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss: Baseline vs Best Model')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f42f39-de25-4aeb-b65e-6557aac36110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "\n",
    "best_model = best_model_results['model']\n",
    "best_model.eval()\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X, y in validation_loader:\n",
    "        pred = best_model(X)\n",
    "        all_preds.extend(pred.argmax(1).cpu().numpy())\n",
    "        all_labels.extend(y.cpu().numpy())\n",
    "\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "disp.plot(cmap='Blues', values_format='d')\n",
    "plt.title('Confusion Matrix: Best Model on Validation Set')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n All visualizations generated successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
