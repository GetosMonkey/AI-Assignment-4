{
 "cells": [
  {
   "cell_type": "raw",
   "id": "a1483c2f-a33b-4585-b4e0-b4e79568dd2a",
   "metadata": {},
   "source": [
    "# CSC2042S 2025 \n",
    "## Assignment 3 \n",
    "\n",
    "## Optimization of a neural network for image classification \n",
    "#### Maryam Abrahams (ABRMAR043)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c59b4218-abd5-4be1-ae68-1c9e3f66bd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Imports and setup \n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random as random\n",
    "\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Reproducibility setup\n",
    "\n",
    "seed = 69\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe7fc9e-d7f2-43e1-8899-f4941aa3a5d1",
   "metadata": {},
   "source": [
    "## Task 1: Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dff462be-42e1-4d81-b3ac-ed05bd930ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processing complete. DataLoaders are ready.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. Define the transformations\n",
    "# ToTensor() converts images to PyTorch Tensors\n",
    "# Normalize() scales the tensor values to a specific range (mean=0.5, std=0.5)\n",
    "transform = Compose([\n",
    "    ToTensor(),\n",
    "    Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# downloading training and \n",
    "train_dataset = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "validation_dataset = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# creating dataLoader objects\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(\"Data processing complete. DataLoaders are ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b230fd26-6c9f-43d8-b28e-f809818cb26f",
   "metadata": {},
   "source": [
    "## Task 2: Building and Training a Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4211cbf7-fbb8-4b91-9b4a-9fb025ce2056",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BaselineMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # nn.Flatten() converts the 28x28 image into a 1D array of 784 pixels\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # Define the sequence of layers\n",
    "        self.network_stack = nn.Sequential(\n",
    "            # Input layer implicitly takes 784 features\n",
    "            nn.Linear(28*28, 128),  # First hidden layer with 128 neurons\n",
    "            nn.ReLU(),             # ReLU activation function\n",
    "            nn.Linear(128, 64),    # Second hidden layer with 64 neurons\n",
    "            nn.ReLU(),             # ReLU activation function\n",
    "            nn.Linear(64, 10)      # Output layer with 10 neurons (for 10 classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Define the forward pass\n",
    "        x = self.flatten(x)\n",
    "        logits = self.network_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8c9f5ff-c7ca-4639-be4e-ed9143557bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train() # Set the model to training mode\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # 1. Forward pass: compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # 2. Backpropagation\n",
    "        optimizer.zero_grad() # Reset gradients from previous iteration\n",
    "        loss.backward()       # Calculate gradients\n",
    "        optimizer.step()      # Update model weights\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "# Function to evaluate the model\n",
    "def validation_loop(dataloader, model, loss_fn):\n",
    "    model.eval() # Set the model to evaluation mode\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    val_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad(): # No need to calculate gradients during evaluation\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            val_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    val_loss /= num_batches\n",
    "    correct /= size\n",
    "    accuracy = 100 * correct\n",
    "    print(f\"Validation Error: \\n Accuracy: {accuracy:>0.1f}%, Avg loss: {val_loss:>8f} \\n\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ffd4deb-87c1-4c14-b6df-134df039ad40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.313334  [   64/60000]\n",
      "loss: 0.651184  [ 6464/60000]\n",
      "loss: 0.690483  [12864/60000]\n",
      "loss: 0.523806  [19264/60000]\n",
      "loss: 0.435783  [25664/60000]\n",
      "loss: 0.407183  [32064/60000]\n",
      "loss: 0.546594  [38464/60000]\n",
      "loss: 0.552970  [44864/60000]\n",
      "loss: 0.491835  [51264/60000]\n",
      "loss: 0.343418  [57664/60000]\n",
      "Validation Error: \n",
      " Accuracy: 84.6%, Avg loss: 0.431423 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.258353  [   64/60000]\n",
      "loss: 0.412075  [ 6464/60000]\n",
      "loss: 0.336064  [12864/60000]\n",
      "loss: 0.329296  [19264/60000]\n",
      "loss: 0.347003  [25664/60000]\n",
      "loss: 0.377744  [32064/60000]\n",
      "loss: 0.283524  [38464/60000]\n",
      "loss: 0.357285  [44864/60000]\n",
      "loss: 0.441620  [51264/60000]\n",
      "loss: 0.547375  [57664/60000]\n",
      "Validation Error: \n",
      " Accuracy: 85.6%, Avg loss: 0.397980 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.405445  [   64/60000]\n",
      "loss: 0.489626  [ 6464/60000]\n",
      "loss: 0.162282  [12864/60000]\n",
      "loss: 0.339373  [19264/60000]\n",
      "loss: 0.468770  [25664/60000]\n",
      "loss: 0.276814  [32064/60000]\n",
      "loss: 0.351786  [38464/60000]\n",
      "loss: 0.310805  [44864/60000]\n",
      "loss: 0.256670  [51264/60000]\n",
      "loss: 0.496602  [57664/60000]\n",
      "Validation Error: \n",
      " Accuracy: 86.2%, Avg loss: 0.383774 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.279392  [   64/60000]\n",
      "loss: 0.249115  [ 6464/60000]\n",
      "loss: 0.338729  [12864/60000]\n",
      "loss: 0.200692  [19264/60000]\n",
      "loss: 0.208765  [25664/60000]\n",
      "loss: 0.412310  [32064/60000]\n",
      "loss: 0.163541  [38464/60000]\n",
      "loss: 0.304826  [44864/60000]\n",
      "loss: 0.221006  [51264/60000]\n",
      "loss: 0.519578  [57664/60000]\n",
      "Validation Error: \n",
      " Accuracy: 86.5%, Avg loss: 0.366325 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.340549  [   64/60000]\n",
      "loss: 0.111117  [ 6464/60000]\n",
      "loss: 0.434655  [12864/60000]\n",
      "loss: 0.288782  [19264/60000]\n",
      "loss: 0.464645  [25664/60000]\n",
      "loss: 0.291117  [32064/60000]\n",
      "loss: 0.285673  [38464/60000]\n",
      "loss: 0.358477  [44864/60000]\n",
      "loss: 0.207517  [51264/60000]\n",
      "loss: 0.373399  [57664/60000]\n",
      "Validation Error: \n",
      " Accuracy: 87.3%, Avg loss: 0.355373 \n",
      "\n",
      "Baseline training complete!\n",
      "Final Baseline Accuracy: 87.28%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "baseline_model = BaselineMLP()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(baseline_model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model for 5 epochs - 10 was way too much\n",
    "epochs = 5\n",
    "baseline_accuracy = 0\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_loader, baseline_model, loss_fn, optimizer)\n",
    "    baseline_accuracy = validation_loop(validation_loader, baseline_model, loss_fn)\n",
    "\n",
    "print(\"Baseline training complete!\")\n",
    "print(f\"Final Baseline Accuracy: {baseline_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb9a91a-e928-488b-a934-9e5c75f39ecf",
   "metadata": {},
   "source": [
    "## Task 3: Hyperparameter Optimization Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303087e1-df53-4b2d-a23a-7d7820ca0bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training with lr=0.1, optimizer=Adam, neurons=64 ---\n",
      "loss: 2.301333  [   64/60000]\n",
      "loss: 2.140321  [ 6464/60000]\n",
      "loss: 1.466400  [12864/60000]\n",
      "loss: 1.747969  [19264/60000]\n",
      "loss: 1.755101  [25664/60000]\n",
      "loss: 2.489678  [32064/60000]\n",
      "loss: 2.292679  [38464/60000]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "learning_rates = [0.1, 0.01, 0.001]\n",
    "optimizers_to_test = [torch.optim.Adam, torch.optim.SGD, torch.optim.RMSprop]\n",
    "neuron_counts = [64, 128, 256] # Example for tuning hidden layer 1 neurons\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "\n",
    "# 2. Create the nested loops to iterate through all combinations\n",
    "for lr in learning_rates:\n",
    "    for opt_class in optimizers_to_test:\n",
    "        for neurons in neuron_counts:\n",
    "            print(f\"--- Training with lr={lr}, optimizer={opt_class.__name__}, neurons={neurons} ---\")\n",
    "            \n",
    "            # NOTE: You will need to modify your model class to accept the neuron count\n",
    "            # as an argument, or create a new class for this experiment.\n",
    "            # For simplicity, let's assume a modified model class `CustomMLP(neurons)`.\n",
    "            # model = CustomMLP(neurons) \n",
    "            \n",
    "            model = BaselineMLP() # Using baseline for structure, but you'd adapt this\n",
    "            \n",
    "            # Instantiate the optimizer with the current learning rate\n",
    "            optimizer = opt_class(model.parameters(), lr=lr)\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            \n",
    "            # Train for the specified number of epochs, 5 - 10 was way too much\n",
    "            epochs = 5\n",
    "            final_accuracy = 0\n",
    "            for t in range(epochs):\n",
    "                # (You would call your train_loop and validation_loop here)\n",
    "                # For brevity, I'm skipping the printouts inside the loop\n",
    "                train_loop(train_loader, model, loss_fn, optimizer)\n",
    "                final_accuracy = validation_loop(validation_loader, model, loss_fn)\n",
    "\n",
    "            # 3. Store the results\n",
    "            results.append({\n",
    "                'learning_rate': lr,\n",
    "                'optimizer': opt_class.__name__,\n",
    "                'hidden_neurons_1': neurons,\n",
    "                'final_accuracy': final_accuracy\n",
    "            })\n",
    "\n",
    "# 4. Convert results to a Pandas DataFrame for easy analysis\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c28b45-65a9-4f4a-8342-98f90a2e50b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
